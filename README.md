# ğŸ¤Ÿ LESAI - Traductor de Lengua de SeÃ±as con IA

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)
![OpenCV](https://img.shields.io/badge/OpenCV-4.x-green.svg)
![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10+-red.svg)
![PyQt5](https://img.shields.io/badge/PyQt5-5.15+-purple.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

**Un traductor inteligente de Lengua de SeÃ±as utilizando visiÃ³n por computadora e inteligencia artificial**



</div>

---
---

## ğŸ¯ DescripciÃ³n

**LESAI** es una aplicaciÃ³n innovadora que utiliza **inteligencia artificial** y **visiÃ³n por computadora** para traducir en tiempo real la **Lengua de SeÃ±as  (LESAI)** a texto y voz. El proyecto combina tecnologÃ­as de vanguardia como **MediaPipe**, **TensorFlow** y **OpenCV** para ofrecer una soluciÃ³n accesible que facilita la comunicaciÃ³n entre personas sordas y oyentes.

### ğŸŒŸ Â¿Por quÃ© LESAI?

- **ğŸ¯ PrecisiÃ³n**: Utiliza redes neuronales avanzadas para reconocimiento preciso de seÃ±as
- **âš¡ Tiempo Real**: TraducciÃ³n instantÃ¡nea sin demoras perceptibles
- **ğŸ¨ Interfaz Moderna**: DiseÃ±o intuitivo y atractivo con tema oscuro profesional
- **ğŸ“Š AnÃ¡lisis Avanzado**: Muestra confianza de predicciones y estadÃ­sticas detalladas
- **ğŸ”Š SÃ­ntesis de Voz**: Convierte el texto traducido a audio usando TTS
- **ğŸ“± FÃ¡cil de Usar**: Interfaz simple que cualquier persona puede usar

---

## âœ¨ CaracterÃ­sticas

### ğŸš€ Funcionalidades Principales

| CaracterÃ­stica | DescripciÃ³n |
|----------------|-------------|
| **ğŸ¥ Captura en Tiempo Real** | Procesamiento de video desde cÃ¡mara web en tiempo real |
| **ğŸ¤– IA Avanzada** | Red neuronal entrenada especÃ­ficamente para LSA |
| **ğŸ“Š AnÃ¡lisis de Confianza** | VisualizaciÃ³n del nivel de certeza de cada predicciÃ³n |
| **ğŸ“ Historial Completo** | Registro de todas las traducciones de la sesiÃ³n |
| **ğŸ”Š SÃ­ntesis de Voz** | ConversiÃ³n automÃ¡tica de texto a audio |
| **ğŸ“ˆ EstadÃ­sticas** | MÃ©tricas detalladas de uso y rendimiento |
| **ğŸ¨ Interfaz Moderna** | DiseÃ±o profesional con tema oscuro |
| **âš™ï¸ FÃ¡cil ConfiguraciÃ³n** | InstalaciÃ³n y configuraciÃ³n simplificadas |

### ğŸ¯ SeÃ±as Reconocidas

Actualmente el sistema reconoce las siguientes seÃ±as bÃ¡sicas de LSA:

- ğŸ‘‹ **"Hola"** - Saludo bÃ¡sico
- ğŸ™ **"Gracias"** - ExpresiÃ³n de agradecimiento  
- ğŸ‘‹ **"AdiÃ³s"** - Despedida

> **Nota**: El sistema estÃ¡ diseÃ±ado para ser expandible. Puedes agregar mÃ¡s seÃ±as entrenando el modelo con nuevos datos.

---

## ğŸ—ï¸ Arquitectura

```mermaid
graph TD
    A[ğŸ“¹ CÃ¡mara Web] --> B[OpenCV]
    B --> C[MediaPipe Hand Detection]
    C --> D[ExtracciÃ³n de Keypoints]
    D --> E[Secuencia de 10 Frames]
    E --> F[ğŸ§  Modelo TensorFlow]
    F --> G[PredicciÃ³n + Confianza]
    G --> H[ğŸ“± Interfaz PyQt5]
    G --> I[ğŸ”Š Text-to-Speech]
    H --> J[ğŸ“Š VisualizaciÃ³n + EstadÃ­sticas]
```

### ğŸ§  Componentes TÃ©cnicos

- **ğŸ¥ Captura de Video**: OpenCV para manejo de cÃ¡mara web
- **ğŸ‘‹ DetecciÃ³n de Manos**: MediaPipe para localizaciÃ³n precisa de manos
- **ï¿½ ExtracciÃ³n de CaracterÃ­sticas**: 216 puntos clave por frame (21 puntos Ã— 2 manos Ã— 3 coordenadas + pose)
- **ğŸ§  Modelo de IA**: Red neuronal densa entrenada con secuencias temporales
- **ğŸ–¥ï¸ Interfaz de Usuario**: PyQt5 con diseÃ±o moderno y responsivo
- **ğŸ”Š Audio**: pyttsx3 para sÃ­ntesis de voz en espaÃ±ol

---

## âš™ï¸ Requisitos del Sistema

### ğŸ’» Hardware MÃ­nimo

- **Procesador**: Intel i3 / AMD equivalente o superior
- **Memoria RAM**: 4 GB mÃ­nimo (8 GB recomendado)
- **CÃ¡mara Web**: ResoluciÃ³n mÃ­nima 640x480
- **Espacio en Disco**: 2 GB libres
- **GPU**: Opcional (acelera el procesamiento con TensorFlow-GPU)

### ğŸ–¥ï¸ Sistemas Operativos Soportados

- âœ… **Windows 10/11** (Recomendado)
- âœ… **macOS 10.14+**
- âœ… **Linux Ubuntu 18.04+**

---

## ğŸš€ InstalaciÃ³n

### ğŸ“‹ Paso 1: Requisitos Previos

AsegÃºrate de tener **Python 3.11+** instalado:

```bash
python --version
```

### ğŸ“¦ Paso 2: Clonar el Repositorio

```bash
git clone https://github.com/tu-usuario/LESAI.git
cd LESAI
```

### ğŸ”§ Paso 3: Crear Entorno Virtual

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### ğŸ“š Paso 4: Instalar Dependencias

```bash
pip install -r requirements.txt
```

### âœ… Paso 5: Verificar InstalaciÃ³n

```bash
python src/main_gui.py
```

Si todo estÃ¡ correcto, deberÃ­a abrirse la interfaz grÃ¡fica.

---



### ğŸ“ Persistencia de Datos

Los datos se mantienen entre ejecuciones gracias a los volÃºmenes:
- `./data/` â†’ ImÃ¡genes y keypoints
- `./models/` â†’ Modelos entrenados

---

## ï¿½ï¿½ğŸ“± Uso

### ğŸ¬ Inicio RÃ¡pido

1. **Ejecutar la aplicaciÃ³n**:
   ```bash
   python src/main_gui.py
   ```

2. **Conectar cÃ¡mara**: AsegÃºrate de que tu cÃ¡mara web estÃ© conectada y funcionando

3. **Iniciar traducciÃ³n**: Haz clic en "â–¶ï¸ Iniciar TraducciÃ³n"

4. **Realizar seÃ±as**: Coloca tus manos frente a la cÃ¡mara y realiza las seÃ±as

5. **Ver resultados**: La traducciÃ³n aparecerÃ¡ en tiempo real en el panel derecho

### ğŸ“Š Interpretando los Resultados

- **ğŸŸ¢ Verde**: PredicciÃ³n exitosa (confianza > 70%)
- **ğŸŸ  Naranja**: Analizando seÃ±as en progreso
- **ğŸ”´ Rojo**: Error o confianza muy baja
- **âš« Gris**: Sistema detenido

### ğŸ¯ Tips para Mejores Resultados

- ğŸ’¡ **IluminaciÃ³n**: Usa buena iluminaciÃ³n, evita contraluz
- ğŸ¤² **PosiciÃ³n**: MantÃ©n las manos claramente visibles
- â±ï¸ **Tiempo**: Realiza las seÃ±as de forma clara y pausada
- ğŸ“ **Distancia**: Mantente a 60-80 cm de la cÃ¡mara
- ğŸ¯ **Fondo**: Usa fondos simples y contrastantes

---

## ğŸ—‚ï¸ Estructura del Proyecto

```
LESAI/
â”œâ”€â”€ ğŸ“ assets/                  # Recursos grÃ¡ficos
â”‚   â””â”€â”€ ğŸ–¼ï¸ LS.png              # Logo del proyecto
â”œâ”€â”€ ğŸ“ data/                    # Datos de entrenamiento
â”‚   â”œâ”€â”€ ğŸ“ frame_actions/       # ImÃ¡genes por acciÃ³n y secuencia
â”‚   â”‚   â”œâ”€â”€ ğŸ“ adios/          # SeÃ±as de "adiÃ³s"
â”‚   â”‚   â”œâ”€â”€ ğŸ“ gracias/        # SeÃ±as de "gracias"
â”‚   â”‚   â””â”€â”€ ğŸ“ hola/           # SeÃ±as de "hola"
â”‚   â””â”€â”€ ğŸ“ keypoints/          # Puntos clave extraÃ­dos
â”‚       â”œâ”€â”€ ğŸ“ adios/          # Keypoints de "adiÃ³s"
â”‚       â”œâ”€â”€ ğŸ“ gracias/        # Keypoints de "gracias"
â”‚       â””â”€â”€ ğŸ“ hola/           # Keypoints de "hola"
â”œâ”€â”€ ğŸ“ docs/                    # DocumentaciÃ³n adicional
â”œâ”€â”€ ğŸ“ scripts/                 # Scripts de utilidad
â”œâ”€â”€ ğŸ“ src/                     # CÃ³digo fuente principal
â”‚   â”œâ”€â”€ ğŸ® main_gui.py         # Interfaz principal
â”‚   â”œâ”€â”€ ğŸ“¸ capture_samples.py  # Captura de muestras
â”‚   â”œâ”€â”€ ğŸ”¢ create_keypoints.py # ExtracciÃ³n de keypoints
â”‚   â”œâ”€â”€ ğŸ‹ï¸ train_model.py      # Entrenamiento del modelo
â”‚   â””â”€â”€ ğŸ“ models/             # Modelos entrenados
â”‚       â”œâ”€â”€ ğŸ§  actions.keras   # Modelo principal
â”‚       â””â”€â”€ ğŸ“‹ label_map.json  # Mapeo de etiquetas
â”œâ”€â”€ ğŸ“ tests/                   # Scripts de prueba
â”œâ”€â”€ ğŸ“ utils/                   # Utilidades del proyecto
â”‚   â”œâ”€â”€ ğŸ“ constants.py        # Constantes del proyecto
â”‚   â””â”€â”€ ğŸ”Š text_to_speech.py   # SÃ­ntesis de voz
â”œâ”€â”€ ğŸ“‹ requirements.txt         # Dependencias de Python
â”œâ”€â”€ ğŸ™ˆ .gitignore              # Archivos ignorados por Git
â””â”€â”€ ğŸ“– README.md               # Este archivo
```

---

## ğŸ”§ ConfiguraciÃ³n

### âš™ï¸ ParÃ¡metros Principales

Puedes modificar estos parÃ¡metros en `utils/constants.py`:

```python
# ConfiguraciÃ³n de secuencias  
SEQUENCE_LENGTH = 10               # Frames por secuencia
KEYPOINT_DIM = 216                # DimensiÃ³n de keypoints
NO_SEQUENCES = 30                 # Muestras por acciÃ³n

# SeÃ±as reconocidas
ACTIONS = ['gracias', 'hola', 'adios']

# Rutas del proyecto
DATA_PATH = 'data/frame_actions'
KEYPOINTS_PATH = 'data/keypoints' 
MODELS_PATH = 'src/models'
```

### ğŸ¨ PersonalizaciÃ³n de Tema

Modifica los colores en `main_gui.py`:


## ğŸ“Š Modelo y Datos

### ğŸ§  Arquitectura del Modelo

- **Entrada**: Secuencias de 10 frames Ã— 216 keypoints (actualizado)
- **Capas Ocultas**: Red neuronal con capas LSTM para procesamiento secuencial
- **ActivaciÃ³n**: ReLU en capas ocultas, Softmax en salida
- **Optimizador**: Adam con learning rate adaptativo
- **PÃ©rdida**: Categorical Crossentropy
- **Clases**: 3 seÃ±as (hola, gracias, adiÃ³s)



### ğŸ“š Expandir el Dataset

Para agregar nuevas seÃ±as al modelo:

1. **Capturar muestras nuevas**:

   ```bash
   python -m capture_samples.py
   ```

2. **Procesar keypoints**:

   ```bash
   python src/create_keypoints.py
   ```

3. **Reentrenar el modelo**:

   ```bash
   python src/train_model.py
   ```

El modelo se guardarÃ¡ automÃ¡ticamente en `src/models/` y estarÃ¡ listo para usar.


### ğŸ¯ Ãreas de Mejora

- [ ] Soporte para mÃ¡s seÃ±as LSA (en progreso)
- [ ] OptimizaciÃ³n de rendimiento con GPU
- [ ] Soporte para mÃºltiples idiomas de seÃ±as
- [ ] IntegraciÃ³n con dispositivos mÃ³viles
- [ ] Mejoras en la precisiÃ³n del modelo con mÃ¡s datos
- [ ] Reconocimiento de expresiones faciales
- [ ] Modo de prÃ¡ctica interactivo

---

## ğŸ‘¥ Autores

<div align="center">


</div>


## ğŸš€ Roadmap

### ğŸ“… VersiÃ³n 2.0 (PrÃ³ximamente)

- [ ] ğŸ¯ **100+ seÃ±as LSA** nuevas
- [ ] ğŸ“± **AplicaciÃ³n mÃ³vil** (Android/iOS)
- [ ] ğŸŒ **Interfaz web** con WebRTC  
- [ ] ğŸ”„ **TraducciÃ³n bidireccional** (texto a seÃ±as)
- [ ] ğŸ“ **Modo educativo** con lecciones interactivas
- [ ] ğŸ¤– **IA mejorada** con transformers

### ğŸ“… VersiÃ³n 1.5 (En desarrollo)

- [ ] ğŸ¨ **Temas personalizables**
- [ ] ğŸ“Š **Dashboard de estadÃ­sticas avanzado**
- [ ] ğŸ’¾ **Exportar historial** a PDF/CSV
- [ ] ğŸ”Š **ConfiguraciÃ³n de voz** (velocidad, tono)
- [ ] ğŸ¥ **GrabaciÃ³n de sesiones**

---

<div align="center">

**Hecho con ğŸ’™ para hacer la comunicaciÃ³n mÃ¡s accesible**

</div>
